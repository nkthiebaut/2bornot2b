{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "raw_corpus = open('corpus.txt', 'r', encoding='utf-8-sig').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length after preprocessing:\n",
      "- 3301707 characters\n",
      "- 585838 words\n"
     ]
    }
   ],
   "source": [
    "title_match_regex = '\\n{3,}\\s+THE SECRET CACHE\\n{3,}.*' # used to remove headers, toc, etc.\n",
    "corpus = re.search(title_match_regex, raw_corpus, flags=re.M+re.S).group()\n",
    "corpus = corpus.replace('\\n', ' ') \n",
    "corpus = re.sub(r' {2,}', ' ', corpus) # replace multiple blanks by one\n",
    "corpus = corpus.replace('----', '') # remove consecutive hyphens that we'll as a tag for the be verb\n",
    "print('Corpus length after preprocessing:')\n",
    "print('- {} characters\\n- {} words'.format(len(corpus), len(corpus.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus first 100 first words: \n",
      " THE SECRET CACHE I THE BIRCH BARK LETTER On the river bank a boy sat watching the slender birch canoes bobbing about in the swift current. The fresh wind reddened his cheeks and the roaring of the rapids filled his ears. Eagerly his eyes followed the movements of the canoes daringly poised in the stream just below the tossing, foaming, white water. It was the first day of the spring fishing, and more exciting sport than this Indian white-fishing Hugh Beaupré had never seen. Three canoes were engaged in the fascinating game, two Indians in each. One knelt in the\n"
     ]
    }
   ],
   "source": [
    "print('Corpus first 100 first words: \\n {}'.format(' '.join(corpus.split()[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"be\" verb occurences: 18647\n",
      "Targets distribution:\n",
      "\tam: 255\n",
      "\tare: 1056\n",
      "\tbe: 2483\n",
      "\tbeen: 1404\n",
      "\tbeing: 327\n",
      "\tis: 2867\n",
      "\twas: 8012\n",
      "\twere: 2243\n"
     ]
    }
   ],
   "source": [
    "be_forms = ['am','are','were','was','is','been','being','be']\n",
    "substitute = '----'\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "def find_targets(tokens):\n",
    "    return [t for t in tokens if t in be_forms]\n",
    "    \n",
    "def remove_targets(tokens):\n",
    "    \"\"\" Replace targets with a substitute in a tokenized text\"\"\"\n",
    "    return [substitute if t in be_forms else t for t in tokens]\n",
    "\n",
    "targets = find_targets(tokens)\n",
    "\n",
    "tokens = remove_targets(tokens)\n",
    "\n",
    "\n",
    "def create_windows(tokens, window_size=5):\n",
    "    \"\"\" Create windows surrouding be forms. \"\"\"\n",
    "    contexts = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word == substitute:\n",
    "            window = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]\n",
    "            window = ' '.join(window)\n",
    "            contexts.append(window)    \n",
    "    return contexts\n",
    "\n",
    "contexts = create_windows(tokens)\n",
    "print('Number of \"be\" verb occurences: {}'.format(len(targets)))\n",
    "print('Targets distribution:')\n",
    "counts = np.unique(targets, return_counts=True)\n",
    "#for form, count in zip(counts[0], counts[1]):\n",
    "#    print('\\t{}: {}'.format(form, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace target names with integer label\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contexts_train, contexts_test, y_train, y_test = train_test_split(contexts, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8457707631014404\n",
      "Test score: 0.633958891867739\n"
     ]
    }
   ],
   "source": [
    "# Vectorize context features\n",
    "#vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, stop_words='english')\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "pipe = make_pipeline(vectorizer, classifier)\n",
    "\n",
    "pipe.fit(contexts_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(contexts_test)\n",
    "\n",
    "print('Train score: {}'.format(accuracy_score(pipe.predict(contexts_train), y_train)))\n",
    "print('Test score: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "sample = open('sample_input.txt', 'r', encoding='utf-8-sig').read().splitlines()\n",
    "N = int(sample[0])\n",
    "sample = sample[1]\n",
    "sample = wordpunct_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from STDIN (for submission on HackerRank)\n",
    "import fileinput\n",
    "\n",
    "f = fileinput.input()\n",
    "N = int(f.readline())\n",
    "sample = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contexts_sample = create_windows(sample)\n",
    "sample_pred = le.inverse_transform(pipe.predict(contexts_sample))\n",
    "print('\\n'.join(sample_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:hackerrank]",
   "language": "python",
   "name": "conda-env-hackerrank-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
