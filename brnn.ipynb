{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import wordpunct_tokenize, FreqDist\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from rnn import softmax, CustomRNN, train_with_sgd\n",
    "\n",
    "raw_corpus = open('corpus.txt', 'r', encoding='utf-8-sig').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training corpus pre-processing\n",
    "A single book text file is provided for the traning corpus. Pre-processing is very light here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length after preprocessing:\n",
      "- 3301707 characters\n",
      "- 585838 words\n"
     ]
    }
   ],
   "source": [
    "## Training corpus pre-processing\n",
    "A single book text file is provided for the traning corpus. Pre-processing is very light here.title_match_regex = '\\n{3,}\\s+THE SECRET CACHE\\n{3,}.*' # used to remove headers, toc, etc.\n",
    "corpus = re.search(title_match_regex, raw_corpus, flags=re.M+re.S).group()\n",
    "corpus = corpus.replace('\\n', ' ') \n",
    "corpus = re.sub(r' {2,}', ' ', corpus) # replace multiple blanks by one\n",
    "corpus = corpus.replace('----', '') # remove consecutive hyphens that we'll as a tag for the be verb\n",
    "print('Corpus length after preprocessing:')\n",
    "print('- {} characters\\n- {} words'.format(len(corpus), len(corpus.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set creation\n",
    "For the training set creation \n",
    "\n",
    "* targets are extracted (\"be\" forms)\n",
    "* targets are removed from text\n",
    "* a small text window (typically 10 words) is extracted around each target. This small text will be used for features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "be_forms = ['am','are','were','was','is','been','being','be']\n",
    "substitute = '----'\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "def find_targets(tokens):\n",
    "    \"\"\" Return a list of found 'be' formed in a tokenized text \"\"\"\n",
    "    return [t for t in tokens if t in be_forms]\n",
    "    \n",
    "def remove_targets(tokens):\n",
    "    \"\"\" Replace targets with a substitute in a tokenized text\"\"\"\n",
    "    return [substitute if t in be_forms else t for t in tokens]\n",
    "\n",
    "targets = find_targets(tokens)\n",
    "\n",
    "tokens = remove_targets(tokens)\n",
    "\n",
    "def create_windows(tokens, window_size=5):\n",
    "    \"\"\" Create windows surrouding be forms. \"\"\"\n",
    "    left_contexts = []\n",
    "    right_contexts = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word == substitute:\n",
    "            window = tokens[i-window_size:i]\n",
    "            window = ' '.join(window)\n",
    "            left_contexts.append(window)\n",
    "            \n",
    "            window = tokens[i+1:i+window_size+1][::-1]\n",
    "            window = ' '.join(window)\n",
    "            right_contexts.append(window)\n",
    "    return left_contexts, right_contexts\n",
    "\n",
    "l_contexts, r_contexts = create_windows(tokens, window_size=10)\n",
    "\n",
    "# Replace target names with integer label\n",
    "target_encoder = LabelEncoder()\n",
    "y = target_encoder.fit_transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute target distribution for later over-sampling\n",
    "distrib = np.bincount(y)\n",
    "prob = 1/distrib[y].astype(float)\n",
    "prob /= prob.sum()\n",
    "\n",
    "left_tokenized_sentences = [c.split() for c in l_contexts]\n",
    "right_tokenized_sentences = [c.split() for c in r_contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18836 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "tokens = left_tokenized_sentences + right_tokenized_sentences\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokens))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(left_tokenized_sentences):\n",
    "    left_tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "for i, sent in enumerate(right_tokenized_sentences):\n",
    "    right_tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Create the training data\n",
    "X_train_left = np.asarray([[word_to_index[w] for w in sent] for sent in left_tokenized_sentences])\n",
    "X_train_right = np.asarray([[word_to_index[w] for w in sent] for sent in right_tokenized_sentences])\n",
    "y_train = np.asarray(target_encoder.fit_transform(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bidirectional-RNN\n",
    "Train a RNN on left-windowed text and an other one on right-windowed text. Their output prediction will used afterwards by a meta-model using prediction from both following and preceeding words. \n",
    "\n",
    "Note that this network is not a Bidirectional RNN stricly speaking, as the two networks are trained separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on preceeding words\n",
      "2017-06-07 01:39:32: Loss after num_examples_seen=0 epoch=0: 2.080189\n",
      "2017-06-07 01:40:03: Loss after num_examples_seen=3500 epoch=5: 1.755644\n",
      "Training a second RNN on following words\n",
      "Score left: 0.439\n",
      "Training RNN on preceeding words\n",
      "2017-06-07 01:40:35: Loss after num_examples_seen=0 epoch=0: 2.079493\n",
      "2017-06-07 01:41:07: Loss after num_examples_seen=3500 epoch=5: 1.853613\n",
      "Training a second RNN on following words\n",
      "Score right: 0.414\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000 # total number of samples used to train RNNs and meta-model\n",
    "n_train_rnn = 7000 # number of samples used to train both RNNs\n",
    "output_dim=len(np.unique(y))\n",
    "\n",
    "# Randomly select examples with over-sampling to handle class imbalance\n",
    "full_mask = np.random.choice(range(len(y)), size=n_samples, replace=True, p=prob)\n",
    "mask = full_mask[:n_train_rnn]\n",
    "mask_blend = full_mask[n_train_rnn:]\n",
    "\n",
    "y_pred = {}\n",
    "y_blend = {}\n",
    "\n",
    "for side, data in [('left', X_train_left), ('right', X_train_right)]:\n",
    "    print('Training RNN on preceeding words')\n",
    "    model = CustomRNN(vocabulary_size, output_dim=output_dim, bptt_truncate=5)\n",
    "    losses = train_with_sgd(model, data[mask], y_train[mask], \n",
    "                            nepoch=10, evaluate_loss_after=5, learning_rate=0.01)\n",
    "    print('Training a second RNN on following words')\n",
    "    y_pred[side] = model.predict_probas(data[mask])\n",
    "    print('Score ' + side + ': %.03f' % accuracy_score(y_train[mask], np.argmax(y_pred[side], axis=1)))\n",
    "    y_blend[side] = model.predict_probas(data[mask_blend])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left-RNN predictions:\n",
      "[('am', 77), ('are', 28), ('be', 5), ('been', 227), ('being', 59), ('is', 105), ('was', 77), ('were', 122)]\n",
      "Right-RNN predictions:\n",
      "[('am', 136), ('are', 73), ('be', 151), ('been', 143), ('is', 166), ('was', 31)]\n"
     ]
    }
   ],
   "source": [
    "# Analyse predictions of left and right RNNs\n",
    "print('Left-RNN predictions:')\n",
    "counts_left = np.unique(target_encoder.inverse_transform(np.argmax(y_pred['left'], axis=1)), return_counts=True)\n",
    "print(list(zip(counts_left[0], counts_left[1])))\n",
    "\n",
    "print('Right-RNN predictions:')\n",
    "counts_right = np.unique(target_encoder.inverse_transform(np.argmax(y_pred['right'], axis=1)), return_counts=True)\n",
    "print(list(zip(counts_right[0], counts_right[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blended meta-model\n",
    "Use predictions from the left RNN and right RNN in a meta-model, to predict class (i.e. the right \"be\" form).\n",
    "\n",
    "The meta-model trains on a held-out training set, unseen while training the RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-model average accuracy: 0.347 \n",
      "Predictions distribution\n",
      "[('am', 38), ('are', 22), ('be', 33), ('been', 44), ('being', 25), ('is', 31), ('was', 4), ('were', 103)]\n"
     ]
    }
   ],
   "source": [
    "# Meta-model input features are the concatenation of RNNs predictions\n",
    "X_blend = np.concatenate((y_blend['left'], y_blend['right']), axis=1)\n",
    "y_true = y[mask_blend]\n",
    "clf = LogisticRegression(C=100.) # Use a large C to avoid strong regularization\n",
    "clf.fit(X_blend, y_true)\n",
    "counts = np.unique(target_encoder.inverse_transform(clf.predict(X_blend)), return_counts=True)\n",
    "print('Meta-model average accuracy: %.3f ' % clf.score(X_blend, y_true))\n",
    "print('Predictions distribution')\n",
    "print(list(zip(counts[0], counts[1])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
