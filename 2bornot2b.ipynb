{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be or what to be, that is the question\n",
    "This notebook gives a solution for the following HackerRank challenge: https://www.hackerrank.com/challenges/to-be-what.\n",
    "\n",
    "The goal is to predict the correct form of the \"be\" verb in a blanked text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "\n",
    "raw_corpus = open('corpus.txt', 'r', encoding='utf-8-sig').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training corpus pre-processing\n",
    "A single book text file is provided for the traning corpus. Pre-processing is very light here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length after preprocessing:\n",
      "- 3301707 characters\n",
      "- 585838 words\n"
     ]
    }
   ],
   "source": [
    "title_match_regex = '\\n{3,}\\s+THE SECRET CACHE\\n{3,}.*' # used to remove headers, toc, etc.\n",
    "corpus = re.search(title_match_regex, raw_corpus, flags=re.M+re.S).group()\n",
    "corpus = corpus.replace('\\n', ' ') \n",
    "corpus = re.sub(r' {2,}', ' ', corpus) # replace multiple blanks by one\n",
    "corpus = corpus.replace('----', '') # remove consecutive hyphens that we'll as a tag for the be verb\n",
    "print('Corpus length after preprocessing:')\n",
    "print('- {} characters\\n- {} words'.format(len(corpus), len(corpus.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus first 100 first words: \n",
      " THE SECRET CACHE I THE BIRCH BARK LETTER On the river bank a boy sat watching the slender birch canoes bobbing about in the swift current. The fresh wind reddened his cheeks and the roaring of the rapids filled his ears. Eagerly his eyes followed the movements of the canoes daringly poised in the stream just below the tossing, foaming, white water. It was the first day of the spring fishing, and more exciting sport than this Indian white-fishing Hugh Beaupré had never seen. Three canoes were engaged in the fascinating game, two Indians in each. One knelt in the\n"
     ]
    }
   ],
   "source": [
    "print('Corpus first 100 first words: \\n {}'.format(' '.join(corpus.split()[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set creation\n",
    "For the training set creation \n",
    "\n",
    "* targets are extracted (\"be\" forms)\n",
    "* targets are removed from text\n",
    "* a small text window (typically 10 words) is extracted around each target. This small text will be used for features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"be\" verb occurences: 18647\n",
      "Targets distribution:\n",
      "[('am', 255), ('are', 1056), ('be', 2483), ('been', 1404), ('being', 327), ('is', 2867), ('was', 8012), ('were', 2243)]\n"
     ]
    }
   ],
   "source": [
    "be_forms = ['am','are','were','was','is','been','being','be']\n",
    "substitute = '----'\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "def find_targets(tokens):\n",
    "    \"\"\" Return a list of found 'be' formed in a tokenized text \"\"\"\n",
    "    return [t for t in tokens if t in be_forms]\n",
    "    \n",
    "def remove_targets(tokens):\n",
    "    \"\"\" Replace targets with a substitute in a tokenized text\"\"\"\n",
    "    return [substitute if t in be_forms else t for t in tokens]\n",
    "\n",
    "targets = find_targets(tokens)\n",
    "\n",
    "tokens = remove_targets(tokens)\n",
    "\n",
    "def create_windows(tokens, window_size=5):\n",
    "    \"\"\" Create windows surrounding be forms. \"\"\"\n",
    "    for i in range(window_size):\n",
    "        tokens.insert(0, 'BEGINNING')\n",
    "    tokens.extend(['END']*window_size)\n",
    "    contexts = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word == substitute:\n",
    "            window = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]\n",
    "            window = ' '.join(window)\n",
    "            contexts.append(window)    \n",
    "    return np.array(contexts).reshape(-1, 1)\n",
    "\n",
    "contexts = create_windows(tokens, window_size=2)\n",
    "print('Number of \"be\" verb occurences: {}'.format(len(targets)))\n",
    "print('Targets distribution:')\n",
    "counts = np.unique(targets, return_counts=True)\n",
    "print(list(zip(counts[0], counts[1])))\n",
    "    \n",
    "# Replace target names with integer label\n",
    "target_encoder = LabelEncoder()\n",
    "y = target_encoder.fit_transform(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Each text window is vectorized with a TF-IDF and modelled with a simple L2 regularized linear regression. Left and right parts of the window are dealt with separately before being concatenated. \n",
    "\n",
    "Note that the targets are strongly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contexts_train, contexts_test, y_train, y_test = train_test_split(contexts, y, test_size=0.3)\n",
    "\n",
    "def avg_accuracy(y_test, y_pred):\n",
    "    \"\"\" Average classes' prediction accuracy. It is a custom score that is not \n",
    "    remniscient of the training set class imbalance. \"\"\"\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    return np.mean(conf_mat.diagonal()/conf_mat.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define function to extract left-part and right-part of the window separately\n",
    "def texts_left(texts):\n",
    "    cut_size = len(texts[0][0].split())//2\n",
    "    return [' '.join(t[0].split()[:cut_size]) for t in texts]\n",
    "\n",
    "def texts_right(texts):\n",
    "    cut_size = len(texts[0][0].split())//2\n",
    "    return [' '.join(t[0].split()[cut_size:]) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy (train): 0.8035946975686428\n",
      "Average accuracy (test): 0.6131110687135597\n"
     ]
    }
   ],
   "source": [
    "left_vectorizer = TfidfVectorizer(max_df=0.5, max_features=3000, min_df=2, stop_words=None)\n",
    "right_vectorizer = TfidfVectorizer(max_df=0.5, max_features=3000, min_df=2, stop_words=None)\n",
    "classifier = LogisticRegression(C=1., class_weight='balanced')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "                 ('left_bow', Pipeline([('cut_left', FunctionTransformer(texts_left)),\n",
    "                                        ('vectorizer_left', left_vectorizer)\n",
    "                                       ]),),\n",
    "                 ('right_bow', Pipeline([('cut_right', FunctionTransformer(texts_right)),\n",
    "                                         ('vectorizer_right', right_vectorizer)]))])),\n",
    "    ('clf', classifier)])\n",
    "\n",
    "pipeline.fit(contexts_train, y_train)\n",
    "\n",
    "y_pred_train = pipeline.predict(contexts_train)\n",
    "y_pred = pipeline.predict(contexts_test)\n",
    "\n",
    "print('Average accuracy (train): {}'.format(avg_accuracy(y_train, y_pred_train)))\n",
    "print('Average accuracy (test): {}'.format(avg_accuracy(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         am       0.16      0.51      0.24        72\n",
      "        are       0.52      0.49      0.50       339\n",
      "         be       0.98      0.92      0.95       780\n",
      "       been       0.98      0.94      0.96       449\n",
      "      being       0.17      0.43      0.24        91\n",
      "         is       0.51      0.49      0.50       828\n",
      "        was       0.71      0.67      0.69      2391\n",
      "       were       0.49      0.46      0.47       645\n",
      "\n",
      "avg / total       0.69      0.66      0.67      5595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "\n",
    "Grid search is used to find the best hyper-parameters for modelling. Should not be used before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "pipeline = Pipeline([('vect', vectorizer), ('clf', classifier)])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_features': (5000, 10000),\n",
    "    'vect__ngram_range': ((1,1), (1,2)),\n",
    "    'clf__C': (0.5, 1.),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring=make_scorer(avg_accuracy))\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(contexts_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "y_pred_train = grid_search.predict(contexts_train)\n",
    "y_pred = grid_search.predict(contexts_test)\n",
    "\n",
    "print('Train score: {}'.format(avg_accuracy(y_train, y_pred_train)))\n",
    "print('Test score: {}'.format(avg_accuracy(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "sample = open('sample_input.txt', 'r', encoding='utf-8-sig').read().splitlines()\n",
    "N = int(sample[0])\n",
    "sample = sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-58c4d99ed3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/hackerrank/lib/python3.5/fileinput.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filelineno\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/hackerrank/lib/python3.5/fileinput.py\u001b[0m in \u001b[0;36m_readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_openhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m  \u001b[0;31m# hide FileInput._readline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "# Read from STDIN (for submission on HackerRank)\n",
    "import fileinput\n",
    "\n",
    "f = fileinput.input()\n",
    "N = int(f.readline())\n",
    "sample = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were\n",
      "was\n",
      "are\n",
      "were\n",
      "is\n",
      "were\n"
     ]
    }
   ],
   "source": [
    "contexts_sample = create_windows(wordpunct_tokenize(sample))\n",
    "sample_pred = target_encoder.inverse_transform(pipeline.predict(contexts_sample))\n",
    "print('\\n'.join(sample_pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:hackerrank]",
   "language": "python",
   "name": "conda-env-hackerrank-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
