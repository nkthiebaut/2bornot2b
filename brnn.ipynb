{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import wordpunct_tokenize, FreqDist\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "raw_corpus = open('corpus.txt', 'r', encoding='utf-8-sig').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length after preprocessing:\n",
      "- 3301707 characters\n",
      "- 585838 words\n"
     ]
    }
   ],
   "source": [
    "title_match_regex = '\\n{3,}\\s+THE SECRET CACHE\\n{3,}.*' # used to remove headers, toc, etc.\n",
    "corpus = re.search(title_match_regex, raw_corpus, flags=re.M+re.S).group()\n",
    "corpus = corpus.replace('\\n', ' ') \n",
    "corpus = re.sub(r' {2,}', ' ', corpus) # replace multiple blanks by one\n",
    "corpus = corpus.replace('----', '') # remove consecutive hyphens that we'll as a tag for the be verb\n",
    "print('Corpus length after preprocessing:')\n",
    "print('- {} characters\\n- {} words'.format(len(corpus), len(corpus.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"be\" verb occurences: 18647\n",
      "Targets distribution:\n",
      "\tam: 255\n",
      "\tare: 1056\n",
      "\tbe: 2483\n",
      "\tbeen: 1404\n",
      "\tbeing: 327\n",
      "\tis: 2867\n",
      "\twas: 8012\n",
      "\twere: 2243\n"
     ]
    }
   ],
   "source": [
    "be_forms = ['am','are','were','was','is','been','being','be']\n",
    "substitute = '----'\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "def find_targets(tokens):\n",
    "    return [t for t in tokens if t in be_forms]\n",
    "    \n",
    "def remove_targets(tokens):\n",
    "    \"\"\" Replace targets with a substitute in a tokenized text\"\"\"\n",
    "    return [substitute if t in be_forms else t for t in tokens]\n",
    "\n",
    "targets = find_targets(tokens)\n",
    "\n",
    "tokens = remove_targets(tokens)\n",
    "\n",
    "def create_windows(tokens, window_size=5):\n",
    "    \"\"\" Create windows surrouding be forms. \"\"\"\n",
    "    left_contexts = []\n",
    "    right_contexts = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word == substitute:\n",
    "            window = tokens[i-window_size:i]\n",
    "            window = ' '.join(window)\n",
    "            left_contexts.append(window)\n",
    "            \n",
    "            window = tokens[i+1:i+window_size+1][::-1]\n",
    "            window = ' '.join(window)\n",
    "            right_contexts.append(window)\n",
    "    return left_contexts, right_contexts\n",
    "\n",
    "l_contexts, r_contexts = create_windows(tokens, window_size=10)\n",
    "\n",
    "# Replace target names with integer label\n",
    "target_encoder = LabelEncoder()\n",
    "y = target_encoder.fit_transform(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute target distribution for later over-sampling\n",
    "distrib = np.bincount(y)\n",
    "prob = 1/distrib[y].astype(float)\n",
    "prob /= prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_tokenized_sentences = [c.split() for c in l_contexts]\n",
    "right_tokenized_sentences = [c.split() for c in r_contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18836 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "tokens = left_tokenized_sentences + right_tokenized_sentences\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokens))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    " \n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(left_tokenized_sentences):\n",
    "    left_tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "for i, sent in enumerate(right_tokenized_sentences):\n",
    "    right_tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Create the training data\n",
    "X_train_left = np.asarray([[word_to_index[w] for w in sent] for sent in left_tokenized_sentences])\n",
    "X_train_right = np.asarray([[word_to_index[w] for w in sent] for sent in right_tokenized_sentences])\n",
    "y_train = np.asarray(target_encoder.fit_transform(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom implementation of RNN and Back-propagation through time (based on [this tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rnn import softmax, CustomRNN, train_with_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bidirectional-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-07 01:13:58: Loss after num_examples_seen=0 epoch=0: 2.080306\n",
      "2017-06-07 01:14:28: Loss after num_examples_seen=3500 epoch=5: 1.700008\n",
      "Score left: 0.369\n",
      "2017-06-07 01:14:59: Loss after num_examples_seen=0 epoch=0: 2.079476\n",
      "2017-06-07 01:15:30: Loss after num_examples_seen=3500 epoch=5: 1.840428\n",
      "Score right: 0.259\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "output_dim=len(np.unique(y))\n",
    "n_samples = 1000\n",
    "n_train_rnn = 700\n",
    "\n",
    "full_mask = np.random.choice(range(len(y)), size=n_samples, replace=True, p=prob)\n",
    "mask = full_mask[:n_train_rnn]\n",
    "mask_blend = full_mask[n_train_rnn:]\n",
    "\n",
    "y_pred = {}\n",
    "y_blend = {}\n",
    "\n",
    "for side, data in [('left', X_train_left), ('right', X_train_right)]:\n",
    "    model = CustomRNN(vocabulary_size, output_dim=output_dim, bptt_truncate=5)\n",
    "    losses = train_with_sgd(model, data[mask], y_train[mask], \n",
    "                            nepoch=10, evaluate_loss_after=5, learning_rate=0.01)\n",
    "    \n",
    "    y_pred[side] = model.predict_probas(data[mask])\n",
    "    print('Score ' + side + ': %.03f' % accuracy_score(y_train[mask], np.argmax(y_pred[side], axis=1)))\n",
    "    y_blend[side] = model.predict_probas(data[mask_blend])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['am', 'are', 'be', 'been', 'being', 'is', 'was'], \n",
       "       dtype='<U5'), array([128, 103,   2, 115, 182, 158,  12]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target_encoder.inverse_transform(np.argmax(y_pred['left'], axis=1)), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['am', 'are', 'been', 'being', 'is', 'was'], \n",
       "       dtype='<U5'), array([ 51, 332, 145,   6, 139,  27]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target_encoder.inverse_transform(np.argmax(y_pred['right'], axis=1)), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blended meta-model\n",
    "Use predictions from the left RNN and right RNN in a meta-model, to predict class (i.e. the right \"be\" form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['am', 'are', 'being', 'were'], \n",
      "      dtype='<U5'), array([ 73,  33,  67, 127]))\n",
      "Score 0.240 \n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "X_blend = np.concatenate((y_blend['left'], y_blend['right']), axis=1)\n",
    "y_true = y[mask_blend]\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_blend, y_true)\n",
    "print(np.unique(target_encoder.inverse_transform(clf.predict(X_blend)), return_counts=True))\n",
    "print('Score %.3f ' % clf.score(X_blend, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
